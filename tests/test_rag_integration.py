#!/usr/bin/env python3
"""
RAGç³»ç»Ÿé›†æˆæµ‹è¯•
æµ‹è¯•BGEå‘é‡åŒ–å™¨ä¸Qdrantæ£€ç´¢å™¨çš„å®Œæ•´é›†æˆ
"""

import time
from typing import List
import numpy as np

from src.embeddings import BGEEmbedder, embed_text
from src.retrievers import QdrantRetriever, create_retriever
from src.model import DocumentChunk, SearchResult


def create_sample_documents() -> List[DocumentChunk]:
    """åˆ›å»ºæµ‹è¯•æ–‡æ¡£æ•°æ®"""
    print("ğŸ“š å‡†å¤‡æµ‹è¯•æ–‡æ¡£...")
    
    # å‡†å¤‡æµ‹è¯•æ–‡æ¡£å†…å®¹ï¼ˆå…³äºéº’éºŸæ“ä½œç³»ç»Ÿçš„ä¿¡æ¯ï¼‰
    test_contents = [
        "éº’éºŸæ“ä½œç³»ç»Ÿæ˜¯ä¸€ä¸ªåŸºäºLinuxå†…æ ¸çš„æ¡Œé¢æ“ä½œç³»ç»Ÿï¼Œä¸“ä¸ºä¸­å›½ç”¨æˆ·è®¾è®¡å¼€å‘ã€‚",
        "Kylin OSå…·æœ‰è‰¯å¥½çš„å®‰å…¨æ€§å’Œç¨³å®šæ€§ï¼Œæ”¯æŒå›½äº§åŒ–è½¯ç¡¬ä»¶ç”Ÿæ€ã€‚",
        "è¯¥ç³»ç»Ÿæä¾›å‹å¥½çš„ç”¨æˆ·ç•Œé¢ï¼Œæ“ä½œç®€å•ç›´è§‚ï¼Œé€‚åˆåŠå…¬å’Œæ—¥å¸¸ä½¿ç”¨ã€‚",
        "éº’éºŸç³»ç»Ÿå†…ç½®äº†ä¸°å¯Œçš„åŠå…¬è½¯ä»¶ï¼ŒåŒ…æ‹¬æ–‡æ¡£ç¼–è¾‘ã€è¡¨æ ¼åˆ¶ä½œå’Œæ¼”ç¤ºå·¥å…·ã€‚",
        "ç³»ç»Ÿæ”¯æŒå¤šç§æ–‡ä»¶æ ¼å¼ï¼Œå…¼å®¹ä¸»æµçš„éŸ³è§†é¢‘æ’­æ”¾å’Œå›¾ç‰‡æŸ¥çœ‹åŠŸèƒ½ã€‚",
        "éº’éºŸæ“ä½œç³»ç»Ÿå…·å¤‡å®Œå–„çš„ç½‘ç»œåŠŸèƒ½ï¼Œæ”¯æŒå„ç§ç½‘ç»œåè®®å’Œè¿æ¥æ–¹å¼ã€‚",
        "ç³»ç»Ÿæä¾›äº†å¼ºå¤§çš„ç³»ç»Ÿç®¡ç†å·¥å…·ï¼Œä¾¿äºç”¨æˆ·è¿›è¡Œç³»ç»Ÿé…ç½®å’Œç»´æŠ¤ã€‚",
        "éº’éºŸOSæ”¯æŒå¤šç§è¾“å…¥æ³•ï¼Œç‰¹åˆ«é’ˆå¯¹ä¸­æ–‡è¾“å…¥è¿›è¡Œäº†ä¼˜åŒ–ã€‚",
        "è¯¥ç³»ç»Ÿå…·æœ‰ä¼˜ç§€çš„ç¡¬ä»¶å…¼å®¹æ€§ï¼Œæ”¯æŒå¤šç§ä¸»æµç¡¬ä»¶è®¾å¤‡ã€‚",
        "éº’éºŸæ“ä½œç³»ç»Ÿå®šæœŸæ›´æ–°ï¼ŒæŒç»­æ”¹è¿›åŠŸèƒ½å’Œä¿®å¤å®‰å…¨æ¼æ´ã€‚"
    ]
    
    print(f"åˆ›å»ºäº† {len(test_contents)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")
    
    # ä½¿ç”¨BGEç”Ÿæˆå‘é‡
    embedder = BGEEmbedder()
    embeddings = embedder.embed(test_contents)
    
    # åˆ›å»ºDocumentChunkå¯¹è±¡
    chunks = []
    for i, (content, embedding) in enumerate(zip(test_contents, embeddings)):
        chunk = DocumentChunk(
            content=content,
            metadata={
                "doc_id": f"doc_{i:03d}",
                "source": "kylin_system_manual",
                "section": f"section_{i // 3 + 1}",  # æ¯3ä¸ªæ–‡æ¡£ä¸€ä¸ªç« èŠ‚
                "content_type": "text",
                "language": "zh-cn",
                "created_at": "2024-01-15T10:00:00Z",
                "word_count": len(content)
            },
            embedding=embedding
        )
        chunks.append(chunk)
    
    print(f"âœ… æˆåŠŸåˆ›å»º {len(chunks)} ä¸ªæ–‡æ¡£å—ï¼Œæ¯ä¸ªå‘é‡ç»´åº¦ä¸º {len(embeddings[0])}")
    return chunks


def test_basic_functionality():
    """æµ‹è¯•åŸºæœ¬åŠŸèƒ½"""
    print("\nğŸ§ª æµ‹è¯•åŸºæœ¬åŠŸèƒ½...")
    
    # åˆ›å»ºæ£€ç´¢å™¨
    retriever = create_retriever(collection_name="test_basic", vector_size=1024)
    
    # åˆ›å»ºæµ‹è¯•æ–‡æ¡£
    chunks = create_sample_documents()
    
    # æ·»åŠ æ–‡æ¡£
    print("ğŸ“¥ æ·»åŠ æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“...")
    start_time = time.time()
    retriever.add_documents(chunks)
    add_time = time.time() - start_time
    print(f"âœ… æ–‡æ¡£æ·»åŠ å®Œæˆï¼Œè€—æ—¶: {add_time:.2f}ç§’")
    
    # æŸ¥çœ‹é›†åˆä¿¡æ¯
    collection_info = retriever.get_collection_info()
    print("ğŸ“Š é›†åˆä¿¡æ¯:")
    for key, value in collection_info.items():
        print(f"  {key}: {value}")
    
    # æµ‹è¯•æ£€ç´¢
    print("\nğŸ” æµ‹è¯•è¯­ä¹‰æ£€ç´¢...")
    test_queries = [
        "éº’éºŸç³»ç»Ÿçš„å®‰å…¨æ€§å¦‚ä½•ï¼Ÿ",
        "ç³»ç»Ÿæ”¯æŒå“ªäº›åŠå…¬åŠŸèƒ½ï¼Ÿ", 
        "å¦‚ä½•è¿›è¡Œç³»ç»Ÿç®¡ç†ï¼Ÿ",
        "æ”¯æŒä»€ä¹ˆè¾“å…¥æ³•ï¼Ÿ"
    ]
    
    embedder = BGEEmbedder()
    
    for query in test_queries:
        print(f"\næŸ¥è¯¢: {query}")
        
        # ç”ŸæˆæŸ¥è¯¢å‘é‡
        query_embedding = embedder.embed(query)[0]
        
        # æ‰§è¡Œæ£€ç´¢
        start_time = time.time()
        results = retriever.search(query_embedding, top_k=3)
        search_time = time.time() - start_time
        
        print(f"æ£€ç´¢è€—æ—¶: {search_time*1000:.1f}æ¯«ç§’")
        print(f"æ‰¾åˆ° {len(results)} ä¸ªç»“æœ:")
        
        for i, result in enumerate(results, 1):
            print(f"  {i}. [ç›¸ä¼¼åº¦: {result.score:.4f}] {result.content[:50]}...")
            print(f"     å…ƒæ•°æ®: doc_id={result.metadata.get('doc_id')}, "
                  f"section={result.metadata.get('section')}")


def test_filtered_search():
    """æµ‹è¯•è¿‡æ»¤æ£€ç´¢"""
    print("\nğŸ§ª æµ‹è¯•è¿‡æ»¤æ£€ç´¢...")
    
    retriever = create_retriever(collection_name="test_filtered", vector_size=1024)
    chunks = create_sample_documents()
    retriever.add_documents(chunks)
    
    embedder = BGEEmbedder()
    query = "ç³»ç»ŸåŠŸèƒ½ä»‹ç»"
    query_embedding = embedder.embed(query)[0]
    
    # ä¸è¿‡æ»¤çš„æ£€ç´¢
    all_results = retriever.search(query_embedding, top_k=5)
    print(f"æ— è¿‡æ»¤æ£€ç´¢: æ‰¾åˆ° {len(all_results)} ä¸ªç»“æœ")
    
    # æŒ‰ç« èŠ‚è¿‡æ»¤
    filtered_results = retriever.search_with_filter(
        query_embedding, 
        top_k=5,
        metadata_filter={"section": "section_1"}
    )
    print(f"ç« èŠ‚1è¿‡æ»¤æ£€ç´¢: æ‰¾åˆ° {len(filtered_results)} ä¸ªç»“æœ")
    
    for result in filtered_results:
        print(f"  - {result.content[:40]}... (section: {result.metadata['section']})")


def test_error_handling():
    """æµ‹è¯•é”™è¯¯å¤„ç†"""
    print("\nğŸ§ª æµ‹è¯•é”™è¯¯å¤„ç†...")
    
    retriever = create_retriever(collection_name="test_errors", vector_size=1024)
    
    # æµ‹è¯•ç©ºæ–‡æ¡£åˆ—è¡¨
    try:
        retriever.add_documents([])
        print("âœ… æ­£ç¡®å¤„ç†ç©ºæ–‡æ¡£åˆ—è¡¨")
    except Exception as e:
        print(f"âŒ å¤„ç†ç©ºæ–‡æ¡£åˆ—è¡¨å¤±è´¥: {e}")
    
    # æµ‹è¯•ç¼ºå°‘embeddingçš„æ–‡æ¡£
    try:
        bad_chunk = DocumentChunk(
            content="æµ‹è¯•å†…å®¹",
            metadata={"test": "value"},
            embedding=None
        )
        retriever.add_documents([bad_chunk])
        print("âŒ åº”è¯¥æ‹’ç»ç¼ºå°‘embeddingçš„æ–‡æ¡£")
    except ValueError as e:
        print(f"âœ… æ­£ç¡®æ‹’ç»ç¼ºå°‘embeddingçš„æ–‡æ¡£: {e}")
    
    # æµ‹è¯•ç»´åº¦ä¸åŒ¹é…çš„æŸ¥è¯¢
    try:
        wrong_dim_vector = [0.1] * 512  # é”™è¯¯çš„ç»´åº¦
        retriever.search(wrong_dim_vector, top_k=3)
        print("âŒ åº”è¯¥æ‹’ç»é”™è¯¯ç»´åº¦çš„æŸ¥è¯¢å‘é‡")
    except ValueError as e:
        print(f"âœ… æ­£ç¡®æ‹’ç»é”™è¯¯ç»´åº¦çš„æŸ¥è¯¢å‘é‡: {e}")
    
    # æµ‹è¯•ç©ºæŸ¥è¯¢å‘é‡
    try:
        retriever.search([], top_k=3)
        print("âŒ åº”è¯¥æ‹’ç»ç©ºæŸ¥è¯¢å‘é‡")
    except ValueError as e:
        print(f"âœ… æ­£ç¡®æ‹’ç»ç©ºæŸ¥è¯¢å‘é‡: {e}")


def test_performance():
    """æµ‹è¯•æ€§èƒ½"""
    print("\nğŸ§ª æµ‹è¯•æ€§èƒ½...")
    
    retriever = create_retriever(collection_name="test_performance", vector_size=1024)
    
    # åˆ›å»ºå¤§é‡æµ‹è¯•æ–‡æ¡£
    print("ç”Ÿæˆå¤§é‡æµ‹è¯•æ–‡æ¡£...")
    large_contents = [
        f"è¿™æ˜¯ç¬¬{i}ä¸ªæµ‹è¯•æ–‡æ¡£ã€‚å†…å®¹åŒ…å«éº’éºŸæ“ä½œç³»ç»Ÿçš„å„ç§åŠŸèƒ½ä»‹ç»ï¼Œ"
        f"åŒ…æ‹¬ä½†ä¸é™äºç³»ç»Ÿç®¡ç†ã€åŠå…¬è½¯ä»¶ã€ç½‘ç»œåŠŸèƒ½ã€å®‰å…¨ç‰¹æ€§ç­‰æ–¹é¢çš„è¯¦ç»†æè¿°ã€‚"
        f"æ–‡æ¡£ç¼–å·ä¸º{i:04d}ï¼Œå±äºæ€§èƒ½æµ‹è¯•çš„ä¸€éƒ¨åˆ†ã€‚"
        for i in range(100)
    ]
    
    embedder = BGEEmbedder()
    print("ç”Ÿæˆå‘é‡...")
    start_time = time.time()
    large_embeddings = embedder.embed(large_contents)
    embedding_time = time.time() - start_time
    print(f"âœ… ç”Ÿæˆ100ä¸ªå‘é‡è€—æ—¶: {embedding_time:.2f}ç§’")
    
    large_chunks = []
    for i, (content, embedding) in enumerate(zip(large_contents, large_embeddings)):
        chunk = DocumentChunk(
            content=content,
            metadata={"doc_id": f"perf_{i:04d}", "batch": "performance_test"},
            embedding=embedding
        )
        large_chunks.append(chunk)
    
    # æ‰¹é‡æ·»åŠ æ–‡æ¡£
    print("æ‰¹é‡æ·»åŠ æ–‡æ¡£...")
    start_time = time.time()
    retriever.add_documents(large_chunks)
    add_time = time.time() - start_time
    print(f"âœ… æ‰¹é‡æ·»åŠ 100ä¸ªæ–‡æ¡£è€—æ—¶: {add_time:.2f}ç§’")
    
    # æ‰¹é‡æ£€ç´¢æµ‹è¯•
    print("æ‰¹é‡æ£€ç´¢æµ‹è¯•...")
    test_queries = [
        "ç³»ç»Ÿç®¡ç†åŠŸèƒ½",
        "åŠå…¬è½¯ä»¶æ”¯æŒ", 
        "ç½‘ç»œè¿æ¥é…ç½®",
        "å®‰å…¨ç‰¹æ€§ä»‹ç»",
        "ç”¨æˆ·ç•Œé¢è®¾è®¡"
    ]
    
    query_embeddings = embedder.embed(test_queries)
    
    start_time = time.time()
    total_results = 0
    for query_emb in query_embeddings:
        results = retriever.search(query_emb, top_k=5)
        total_results += len(results)
    search_time = time.time() - start_time
    
    print(f"âœ… 5æ¬¡æ£€ç´¢æ€»è€—æ—¶: {search_time:.2f}ç§’")
    print(f"âœ… å¹³å‡å•æ¬¡æ£€ç´¢: {search_time/5*1000:.1f}æ¯«ç§’")
    print(f"âœ… æ€»æ£€ç´¢ç»“æœæ•°: {total_results}")
    
    # æ˜¾ç¤ºæœ€ç»ˆé›†åˆçŠ¶æ€
    final_info = retriever.get_collection_info()
    print(f"âœ… æœ€ç»ˆé›†åˆçŠ¶æ€: {final_info['points_count']} ä¸ªæ–‡æ¡£")


def test_integration_scenario():
    """æµ‹è¯•å®Œæ•´çš„RAGåœºæ™¯"""
    print("\nğŸ§ª æµ‹è¯•å®Œæ•´RAGåœºæ™¯...")
    
    # åˆ›å»ºä¸“é—¨çš„æ£€ç´¢å™¨
    retriever = create_retriever(collection_name="rag_demo", vector_size=1024)
    
    # æ¨¡æ‹Ÿæ–‡æ¡£çŸ¥è¯†åº“
    knowledge_base = [
        "éº’éºŸæ“ä½œç³»ç»ŸåŸºäºLinuxå†…æ ¸ï¼Œä¸“ä¸ºä¸­å›½ç”¨æˆ·å®šåˆ¶å¼€å‘ï¼Œå…·æœ‰å®Œå…¨è‡ªä¸»çŸ¥è¯†äº§æƒã€‚",
        "ç³»ç»Ÿé‡‡ç”¨è‡ªä¸»ç ”å‘çš„UKUIæ¡Œé¢ç¯å¢ƒï¼Œç•Œé¢ç®€æ´ç¾è§‚ï¼Œæ“ä½œä½“éªŒå‹å¥½ã€‚",
        "å†…ç½®WPSåŠå…¬å¥—ä»¶ã€æœç‹—è¾“å…¥æ³•ã€ç«ç‹æµè§ˆå™¨ç­‰å¸¸ç”¨è½¯ä»¶ï¼Œæ»¡è¶³æ—¥å¸¸åŠå…¬éœ€æ±‚ã€‚",
        "æ”¯æŒå›½äº§CPUå¦‚é¾™èŠ¯ã€é£è…¾ã€å…†èŠ¯ç­‰ï¼Œä»¥åŠå›½äº§æ˜¾å¡ã€ç½‘å¡ç­‰ç¡¬ä»¶è®¾å¤‡ã€‚",
        "ç³»ç»Ÿå…·å¤‡å¤šé‡å®‰å…¨é˜²æŠ¤æœºåˆ¶ï¼ŒåŒ…æ‹¬è®¿é—®æ§åˆ¶ã€æ•°æ®åŠ å¯†ã€å…¥ä¾µæ£€æµ‹ç­‰åŠŸèƒ½ã€‚",
        "æä¾›åº”ç”¨å•†åº—ï¼Œç”¨æˆ·å¯ä»¥ä¾¿æ·åœ°ä¸‹è½½å®‰è£…å„ç±»åº”ç”¨è½¯ä»¶ã€‚",
        "ç³»ç»Ÿæ”¯æŒè™šæ‹ŸåŒ–æŠ€æœ¯ï¼Œå¯ä»¥è¿è¡ŒWindowsåº”ç”¨ç¨‹åºï¼Œä¿è¯è½¯ä»¶å…¼å®¹æ€§ã€‚",
        "å…·å¤‡å®Œå–„çš„é©±åŠ¨æ”¯æŒï¼Œå…¼å®¹ä¸»æµæ‰“å°æœºã€æ‰«æä»ªã€æ‘„åƒå¤´ç­‰å¤–è®¾ã€‚",
        "æ”¯æŒå¤šç§ç½‘ç»œåè®®ï¼ŒåŒ…æ‹¬æœ‰çº¿ç½‘ç»œã€æ— çº¿WiFiã€è“ç‰™è¿æ¥ç­‰ã€‚",
        "å®šæœŸå‘å¸ƒç³»ç»Ÿæ›´æ–°ï¼Œä¿®å¤å®‰å…¨æ¼æ´ï¼Œå¢åŠ æ–°åŠŸèƒ½ï¼Œä¿æŒç³»ç»Ÿç¨³å®šæ€§ã€‚"
    ]
    
    # æ„å»ºçŸ¥è¯†åº“
    print("æ„å»ºçŸ¥è¯†åº“...")
    embedder = BGEEmbedder()
    kb_embeddings = embedder.embed(knowledge_base)
    
    kb_chunks = []
    for i, (content, embedding) in enumerate(zip(knowledge_base, kb_embeddings)):
        chunk = DocumentChunk(
            content=content,
            metadata={
                "doc_id": f"kb_{i:03d}",
                "source": "kylin_knowledge_base",
                "topic": ["system", "security", "software", "hardware", "network"][i % 5],
                "priority": "high" if i < 5 else "normal"
            },
            embedding=embedding
        )
        kb_chunks.append(chunk)
    
    retriever.add_documents(kb_chunks)
    print(f"âœ… çŸ¥è¯†åº“æ„å»ºå®Œæˆï¼ŒåŒ…å« {len(kb_chunks)} ä¸ªæ¡ç›®")
    
    # æ¨¡æ‹Ÿç”¨æˆ·é—®ç­”
    print("\næ¨¡æ‹Ÿç”¨æˆ·é—®ç­”åœºæ™¯:")
    user_questions = [
        "éº’éºŸç³»ç»Ÿæ”¯æŒå“ªäº›å›½äº§ç¡¬ä»¶ï¼Ÿ",
        "å¦‚ä½•ä¿è¯ç³»ç»Ÿçš„å®‰å…¨æ€§ï¼Ÿ",
        "ç³»ç»Ÿè‡ªå¸¦å“ªäº›åŠå…¬è½¯ä»¶ï¼Ÿ",
        "å¯ä»¥è¿è¡ŒWindowsç¨‹åºå—ï¼Ÿ",
        "å¦‚ä½•è¿æ¥ç½‘ç»œè®¾å¤‡ï¼Ÿ"
    ]
    
    for question in user_questions:
        print(f"\nâ“ ç”¨æˆ·é—®é¢˜: {question}")
        
        # ç”Ÿæˆé—®é¢˜å‘é‡
        question_embedding = embedder.embed(question)[0]
        
        # æ£€ç´¢ç›¸å…³æ–‡æ¡£
        relevant_docs = retriever.search(question_embedding, top_k=2)
        
        print("ğŸ“‹ æ£€ç´¢åˆ°çš„ç›¸å…³ä¿¡æ¯:")
        for i, doc in enumerate(relevant_docs, 1):
            print(f"  {i}. [ç›¸ä¼¼åº¦: {doc.score:.4f}] {doc.content}")
            print(f"     [ä¸»é¢˜: {doc.metadata['topic']}, ä¼˜å…ˆçº§: {doc.metadata['priority']}]")
        
        # åœ¨å®é™…RAGç³»ç»Ÿä¸­ï¼Œè¿™é‡Œä¼šå°†æ£€ç´¢åˆ°çš„å†…å®¹ä¼ é€’ç»™LLMç”Ÿæˆç­”æ¡ˆ
        print("ğŸ’¡ åŸºäºæ£€ç´¢å†…å®¹ï¼Œç³»ç»Ÿå¯ä»¥ç”Ÿæˆé’ˆå¯¹æ€§å›ç­”")


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ BGE + Qdrant RAGé›†æˆæµ‹è¯•")
    print("=" * 60)
    
    start_time = time.time()
    
    try:
        test_basic_functionality()
        test_filtered_search()
        test_error_handling()
        test_performance()
        test_integration_scenario()
        
        total_time = time.time() - start_time
        print(f"\nâœ… æ‰€æœ‰é›†æˆæµ‹è¯•å®Œæˆï¼æ€»è€—æ—¶: {total_time:.2f}ç§’")
        
        print(f"\nğŸ“Š æµ‹è¯•æ€»ç»“:")
        print(f"- âœ… åŸºæœ¬åŠŸèƒ½: BGEå‘é‡åŒ– + Qdrantå­˜å‚¨æ£€ç´¢")
        print(f"- âœ… è¿‡æ»¤æ£€ç´¢: æ”¯æŒå…ƒæ•°æ®è¿‡æ»¤æ¡ä»¶")
        print(f"- âœ… é”™è¯¯å¤„ç†: å®Œå–„çš„å¼‚å¸¸å¤„ç†æœºåˆ¶")
        print(f"- âœ… æ€§èƒ½è¡¨ç°: æ‰¹é‡å¤„ç†é«˜æ•ˆç¨³å®š")
        print(f"- âœ… RAGåœºæ™¯: å®Œæ•´çš„é—®ç­”æ£€ç´¢æµç¨‹")
        print(f"\nğŸ¯ BGE + Qdrant é›†æˆæ–¹æ¡ˆéªŒè¯æˆåŠŸï¼")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
